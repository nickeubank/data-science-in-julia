{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Exercises\n",
    "\n",
    "In these exercises we will work on data from a series of global weather monitoring stations used to measure climate trends to examine long-term trends in temperature for your home locality. This data comes from the Global Historical Climatology Network, and is the actual raw data provided by NOAA. The only changes I have made to this data are a few small formatting changes to help meet the learning goals of this exercise. \n",
    "\n",
    "To do these excercises, first please download the data for this exercise [from here](https://www.dropbox.com/s/xh1zk6rlum9z4pb/ghcnd_all.tar.gz?dl=0). Note this is a big file (this is a big-data exercise, after all), so be patient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1)** The data we'll be working with can be found in the file `ghcnd_daily.tar.gz`. It includes daily weather data from thousands of weather stations around the work over many decades. \n",
    "\n",
    "Begin by unzipping the file and checking it's size -- it should come out to be *about* 30gb, which means there's just no way most students (who usually have, at most, 16gb of RAM) can import this dataset into pandas directly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2)** Thankfully, we aren't going to be working with *all* the data today. Instead, each pair should pick two weather stations to examine during this analysis. \n",
    "\n",
    "To pick your stations, we'll need to open the `ghcnd-stations.txt` file in the directory you've downloaded. It includes both station codes (which is what we'll find in the `ghcnd_daily.tar.gz` data, as well as the name and location of each station). \n",
    "\n",
    "When picking a weather station, make sure to pick on that has an associated \"WMO ID\", as these designate more official stations, ensuring you'll get a station with data that has been recorded over a long period. \n",
    "\n",
    "The `ghcnd-stations.txt` is a \"fixed-width\" dataset, meaning that instead of putting commas or tabs between observations, all columns have the same width (in terms of number of characters). So to import this data you'll have to (a) read the notes about the data in the project README.txt, and (b) read about how to read in fixed-width data in pandas. When entering column specifications, remember that normal people count from 1 and include end points, while Python counts from 0 and doesn't include end points (so if the readme says data is in columns 10-20, in Python that'd be 9 through 20). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.chdir('/users/nick/downloads/global_climate_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [(0, 11),\n",
    "           (12, 20),\n",
    "           (21, 30),\n",
    "           (31, 37),\n",
    "           (38, 40),\n",
    "           (41, 71),\n",
    "           (72, 75),\n",
    "           (76, 79),\n",
    "           (80, 85)]\n",
    "\n",
    "stations = pd.read_fwf('ghcnd-stations.txt', \n",
    "                       colspecs=columns, \n",
    "                       header=None,\n",
    "                       names = [\"ID\", \"LATITUDE\", \"LONGITUDE\", \"ELEVATION\", \n",
    "                                \"STATE\", \"NAME\", \"GSN FLAG\", \"HCN/CRN FLAG\", \n",
    "                                \"WMO ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>ELEVATION</th>\n",
       "      <th>STATE</th>\n",
       "      <th>NAME</th>\n",
       "      <th>GSN FLAG</th>\n",
       "      <th>HCN/CRN FLAG</th>\n",
       "      <th>WMO ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACW00011604</td>\n",
       "      <td>17.1167</td>\n",
       "      <td>-61.7833</td>\n",
       "      <td>10.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ST JOHNS COOLIDGE FLD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ACW00011647</td>\n",
       "      <td>17.1333</td>\n",
       "      <td>-61.7833</td>\n",
       "      <td>19.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ST JOHNS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>25.3330</td>\n",
       "      <td>55.5170</td>\n",
       "      <td>34.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SHARJAH INTER. AIRP</td>\n",
       "      <td>GSN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41196.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AEM00041194</td>\n",
       "      <td>25.2550</td>\n",
       "      <td>55.3640</td>\n",
       "      <td>10.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DUBAI INTL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41194.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AEM00041217</td>\n",
       "      <td>24.4330</td>\n",
       "      <td>54.6510</td>\n",
       "      <td>26.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABU DHABI INTL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41217.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID  LATITUDE  LONGITUDE  ELEVATION STATE                   NAME  \\\n",
       "0  ACW00011604   17.1167   -61.7833       10.1   NaN  ST JOHNS COOLIDGE FLD   \n",
       "1  ACW00011647   17.1333   -61.7833       19.2   NaN               ST JOHNS   \n",
       "2  AE000041196   25.3330    55.5170       34.0   NaN    SHARJAH INTER. AIRP   \n",
       "3  AEM00041194   25.2550    55.3640       10.4   NaN             DUBAI INTL   \n",
       "4  AEM00041217   24.4330    54.6510       26.8   NaN         ABU DHABI INTL   \n",
       "\n",
       "  GSN FLAG HCN/CRN FLAG   WMO ID  \n",
       "0      NaN          NaN      NaN  \n",
       "1      NaN          NaN      NaN  \n",
       "2      GSN          NaN  41196.0  \n",
       "3      NaN          NaN  41194.0  \n",
       "4      NaN          NaN  41217.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stations.head()\n",
    "#stations = stations[stations[7].isin(['HCN', \"CRN\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = stations[stations[\"WMO ID\"].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>ELEVATION</th>\n",
       "      <th>STATE</th>\n",
       "      <th>NAME</th>\n",
       "      <th>GSN FLAG</th>\n",
       "      <th>HCN/CRN FLAG</th>\n",
       "      <th>WMO ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29197</th>\n",
       "      <td>CA006131983</td>\n",
       "      <td>42.8667</td>\n",
       "      <td>-80.550</td>\n",
       "      <td>232.0</td>\n",
       "      <td>ON</td>\n",
       "      <td>DELHI CS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>71573.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39039</th>\n",
       "      <td>IN022021900</td>\n",
       "      <td>28.5830</td>\n",
       "      <td>77.200</td>\n",
       "      <td>216.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NEW DELHI/SAFDARJUN</td>\n",
       "      <td>GSN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42182.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39045</th>\n",
       "      <td>IN022023000</td>\n",
       "      <td>28.5670</td>\n",
       "      <td>77.117</td>\n",
       "      <td>233.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NEW DELHI/PALAM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42181.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID  LATITUDE  LONGITUDE  ELEVATION STATE                 NAME  \\\n",
       "29197  CA006131983   42.8667    -80.550      232.0    ON             DELHI CS   \n",
       "39039  IN022021900   28.5830     77.200      216.0   NaN  NEW DELHI/SAFDARJUN   \n",
       "39045  IN022023000   28.5670     77.117      233.0   NaN      NEW DELHI/PALAM   \n",
       "\n",
       "      GSN FLAG HCN/CRN FLAG   WMO ID  \n",
       "29197      NaN          NaN  71573.0  \n",
       "39039      GSN          NaN  42182.0  \n",
       "39045      NaN          NaN  42181.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stations[stations['NAME'].str.contains(\"DELHI\")] # Let's do one from  New Delhi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>ELEVATION</th>\n",
       "      <th>STATE</th>\n",
       "      <th>NAME</th>\n",
       "      <th>GSN FLAG</th>\n",
       "      <th>HCN/CRN FLAG</th>\n",
       "      <th>WMO ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>AR000875850</td>\n",
       "      <td>-34.583</td>\n",
       "      <td>-58.483</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BUENOS AIRES OBSERV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>87585.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID  LATITUDE  LONGITUDE  ELEVATION STATE                 NAME  \\\n",
       "283  AR000875850   -34.583    -58.483       25.0   NaN  BUENOS AIRES OBSERV   \n",
       "\n",
       "    GSN FLAG HCN/CRN FLAG   WMO ID  \n",
       "283      NaN          NaN  87585.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#IN022023000\n",
    "\n",
    "# and one from argentina\n",
    "stations[stations['NAME'].str.contains(\"BUENOS AIRES\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AR000875850"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(3)** Now that we something about the observations we want to work with, we can now turn to our actual weather data. \n",
    "\n",
    "Before anything else, it's good with these big datasets to just import ~100 lines so you can get a feel for the data. Note that as with our stations data, this dataset is a fixed-width formatted file. Unlike that data, though, this dataset has a *lot* of columns -- too many to write all out by hand. With that in mind, I would suggest that you use some code like the following (this exercise isn't about fixed-width data per se, so I'll try and save you some trouble since you already solved the read-in above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['id', 'year', 'month', 'element']\n",
    "for i in range(1, 32):\n",
    "    column_names.extend([f'value{i}', f'mflag{i}', f'qflag{i}', f'sflag{i}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_specs = [(0, 11), (11, 15), (15, 17), (17, 21)]\n",
    "\n",
    "start = 21              \n",
    "for i in range(0, 31):\n",
    "    local_start = start + (i * 8)\n",
    "    column_specs.append((local_start, local_start+5))\n",
    "    column_specs.append((local_start + 5, local_start + 6)) # MFlag\n",
    "    column_specs.append((local_start + 6, local_start + 7)) # QFlag\n",
    "    column_specs.append((local_start + 7, local_start + 8)) # SFlag\n",
    "\n",
    "# Check I didn't screw up\n",
    "for i in range(len(column_specs)-1):\n",
    "    assert column_specs[i][1] == column_specs[i+1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_fwf('ghcnd_daily.dat', colspecs=column_specs,\n",
    "                 names = column_names, \n",
    "                 nrows = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>element</th>\n",
       "      <th>value1</th>\n",
       "      <th>mflag1</th>\n",
       "      <th>qflag1</th>\n",
       "      <th>sflag1</th>\n",
       "      <th>value2</th>\n",
       "      <th>mflag2</th>\n",
       "      <th>...</th>\n",
       "      <th>qflag29</th>\n",
       "      <th>sflag29</th>\n",
       "      <th>value30</th>\n",
       "      <th>mflag30</th>\n",
       "      <th>qflag30</th>\n",
       "      <th>sflag30</th>\n",
       "      <th>value31</th>\n",
       "      <th>mflag31</th>\n",
       "      <th>qflag31</th>\n",
       "      <th>sflag31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACW00011604</td>\n",
       "      <td>1949</td>\n",
       "      <td>1</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>289</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>X</td>\n",
       "      <td>289</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>X</td>\n",
       "      <td>272</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>X</td>\n",
       "      <td>272</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ACW00011604</td>\n",
       "      <td>1949</td>\n",
       "      <td>1</td>\n",
       "      <td>TMIN</td>\n",
       "      <td>217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>X</td>\n",
       "      <td>228</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>X</td>\n",
       "      <td>206</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>X</td>\n",
       "      <td>217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ACW00011604</td>\n",
       "      <td>1949</td>\n",
       "      <td>1</td>\n",
       "      <td>PRCP</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>X</td>\n",
       "      <td>30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>X</td>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>X</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ACW00011604</td>\n",
       "      <td>1949</td>\n",
       "      <td>1</td>\n",
       "      <td>SNOW</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>X</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>X</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>X</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACW00011604</td>\n",
       "      <td>1949</td>\n",
       "      <td>1</td>\n",
       "      <td>SNWD</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>X</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>X</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>X</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  year  month element  value1 mflag1  qflag1 sflag1  value2  \\\n",
       "0  ACW00011604  1949      1    TMAX     289    NaN     NaN      X     289   \n",
       "1  ACW00011604  1949      1    TMIN     217    NaN     NaN      X     228   \n",
       "2  ACW00011604  1949      1    PRCP       0    NaN     NaN      X      30   \n",
       "3  ACW00011604  1949      1    SNOW       0    NaN     NaN      X       0   \n",
       "4  ACW00011604  1949      1    SNWD       0    NaN     NaN      X       0   \n",
       "\n",
       "  mflag2  ...  qflag29 sflag29  value30 mflag30  qflag30 sflag30  value31  \\\n",
       "0    NaN  ...      NaN       X      272     NaN      NaN       X      272   \n",
       "1    NaN  ...      NaN       X      206     NaN      NaN       X      217   \n",
       "2    NaN  ...      NaN       X       28     NaN      NaN       X       15   \n",
       "3    NaN  ...      NaN       X        0     NaN      NaN       X        0   \n",
       "4    NaN  ...      NaN       X        0     NaN      NaN       X        0   \n",
       "\n",
       "   mflag31  qflag31 sflag31  \n",
       "0      NaN      NaN       X  \n",
       "1      NaN      NaN       X  \n",
       "2      NaN      NaN       X  \n",
       "3      NaN      NaN       X  \n",
       "4      NaN      NaN       X  \n",
       "\n",
       "[5 rows x 128 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(4)** Once you have a sense of the data, write code to read in all the climate data and keep only the observations for the weather stations you've selected to focus on. The only outcome we care about is maximium temp. \n",
    "\n",
    "In addition to your own 2 weather stations, please also include station USC00050848 (a weather station from near my home!) so you can generate results that we can all compare (to check for accuracy). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stations = ['IN022023000', 'AR000875850', 'USC00050848']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With 1,000,000 per chunk: 1.2 minutes per block..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "finished import in 1.37 minutes\n",
      "finished subset in 0.00 minutes\n",
      "1\n",
      "finished import in 1.46 minutes\n",
      "finished subset in 0.00 minutes\n",
      "2\n",
      "finished import in 1.41 minutes\n",
      "finished subset in 0.00 minutes\n",
      "3\n",
      "finished import in 1.47 minutes\n",
      "finished subset in 0.00 minutes\n",
      "4\n",
      "finished import in 1.41 minutes\n",
      "finished subset in 0.00 minutes\n",
      "5\n",
      "finished import in 1.39 minutes\n",
      "finished subset in 0.00 minutes\n",
      "6\n",
      "finished import in 1.38 minutes\n",
      "finished subset in 0.00 minutes\n",
      "7\n",
      "finished import in 1.39 minutes\n",
      "finished subset in 0.00 minutes\n",
      "8\n",
      "finished import in 1.21 minutes\n",
      "finished subset in 0.00 minutes\n",
      "9\n",
      "finished import in 1.16 minutes\n",
      "finished subset in 0.00 minutes\n",
      "10\n",
      "finished import in 1.24 minutes\n",
      "finished subset in 0.00 minutes\n",
      "11\n",
      "finished import in 1.17 minutes\n",
      "finished subset in 0.00 minutes\n",
      "12\n",
      "finished import in 1.21 minutes\n",
      "finished subset in 0.00 minutes\n",
      "13\n",
      "finished import in 1.16 minutes\n",
      "finished subset in 0.00 minutes\n",
      "14\n",
      "finished import in 1.16 minutes\n",
      "finished subset in 0.00 minutes\n",
      "15\n",
      "finished import in 1.19 minutes\n",
      "finished subset in 0.00 minutes\n",
      "16\n",
      "finished import in 1.19 minutes\n",
      "finished subset in 0.00 minutes\n",
      "17\n",
      "finished import in 1.20 minutes\n",
      "finished subset in 0.00 minutes\n",
      "18\n",
      "finished import in 1.17 minutes\n",
      "finished subset in 0.00 minutes\n",
      "19\n",
      "finished import in 1.33 minutes\n",
      "finished subset in 0.00 minutes\n",
      "20\n",
      "finished import in 1.20 minutes\n",
      "finished subset in 0.00 minutes\n",
      "21\n",
      "finished import in 1.19 minutes\n",
      "finished subset in 0.00 minutes\n",
      "22\n",
      "finished import in 1.19 minutes\n",
      "finished subset in 0.00 minutes\n",
      "23\n",
      "finished import in 1.22 minutes\n",
      "finished subset in 0.00 minutes\n",
      "24\n",
      "finished import in 1.20 minutes\n",
      "finished subset in 0.00 minutes\n",
      "25\n",
      "finished import in 1.21 minutes\n",
      "finished subset in 0.00 minutes\n",
      "26\n",
      "finished import in 1.33 minutes\n",
      "finished subset in 0.00 minutes\n",
      "27\n",
      "finished import in 1.20 minutes\n",
      "finished subset in 0.00 minutes\n",
      "28\n",
      "finished import in 1.19 minutes\n",
      "finished subset in 0.00 minutes\n",
      "29\n",
      "finished import in 1.18 minutes\n",
      "finished subset in 0.00 minutes\n",
      "30\n",
      "finished import in 1.22 minutes\n",
      "finished subset in 0.00 minutes\n",
      "31\n",
      "finished import in 1.23 minutes\n",
      "finished subset in 0.00 minutes\n",
      "32\n",
      "finished import in 1.18 minutes\n",
      "finished subset in 0.01 minutes\n",
      "33\n",
      "finished import in 1.16 minutes\n",
      "finished subset in 0.01 minutes\n",
      "34\n",
      "finished import in 1.17 minutes\n",
      "finished subset in 0.01 minutes\n",
      "35\n",
      "finished import in 1.16 minutes\n",
      "finished subset in 0.01 minutes\n",
      "36\n",
      "finished import in 1.31 minutes\n",
      "finished subset in 0.00 minutes\n",
      "37\n",
      "finished import in 1.34 minutes\n",
      "finished subset in 0.00 minutes\n",
      "38\n",
      "finished import in 1.22 minutes\n",
      "finished subset in 0.00 minutes\n",
      "39\n",
      "finished import in 1.24 minutes\n",
      "finished subset in 0.00 minutes\n",
      "40\n",
      "finished import in 1.20 minutes\n",
      "finished subset in 0.00 minutes\n",
      "41\n",
      "finished import in 1.22 minutes\n",
      "finished subset in 0.00 minutes\n",
      "42\n",
      "finished import in 1.16 minutes\n",
      "finished subset in 0.00 minutes\n",
      "43\n",
      "finished import in 1.20 minutes\n",
      "finished subset in 0.00 minutes\n",
      "44\n",
      "finished import in 1.29 minutes\n",
      "finished subset in 0.00 minutes\n",
      "45\n",
      "finished import in 1.23 minutes\n",
      "finished subset in 0.01 minutes\n",
      "46\n",
      "finished import in 1.25 minutes\n",
      "finished subset in 0.00 minutes\n",
      "47\n",
      "finished import in 1.21 minutes\n",
      "finished subset in 0.00 minutes\n",
      "48\n",
      "finished import in 1.18 minutes\n",
      "finished subset in 0.00 minutes\n",
      "49\n",
      "finished import in 1.17 minutes\n",
      "finished subset in 0.00 minutes\n",
      "50\n",
      "finished import in 1.17 minutes\n",
      "finished subset in 0.00 minutes\n",
      "51\n",
      "finished import in 1.18 minutes\n",
      "finished subset in 0.00 minutes\n",
      "52\n",
      "finished import in 1.18 minutes\n",
      "finished subset in 0.00 minutes\n",
      "53\n",
      "finished import in 1.18 minutes\n",
      "finished subset in 0.00 minutes\n",
      "54\n",
      "finished import in 1.18 minutes\n",
      "finished subset in 0.00 minutes\n",
      "55\n",
      "finished import in 1.17 minutes\n",
      "finished subset in 0.00 minutes\n",
      "56\n",
      "finished import in 1.32 minutes\n",
      "finished subset in 0.00 minutes\n",
      "57\n",
      "finished import in 1.22 minutes\n",
      "finished subset in 0.00 minutes\n",
      "58\n",
      "finished import in 1.18 minutes\n",
      "finished subset in 0.00 minutes\n",
      "59\n",
      "finished import in 1.19 minutes\n",
      "finished subset in 0.00 minutes\n",
      "60\n",
      "finished import in 1.17 minutes\n",
      "finished subset in 0.00 minutes\n",
      "61\n",
      "finished import in 1.17 minutes\n",
      "finished subset in 0.00 minutes\n",
      "62\n",
      "finished import in 1.18 minutes\n",
      "finished subset in 0.00 minutes\n",
      "63\n",
      "finished import in 1.19 minutes\n",
      "finished subset in 0.00 minutes\n",
      "64\n",
      "finished import in 1.15 minutes\n",
      "finished subset in 0.00 minutes\n",
      "65\n",
      "finished import in 1.19 minutes\n",
      "finished subset in 0.00 minutes\n",
      "66\n",
      "finished import in 1.17 minutes\n",
      "finished subset in 0.00 minutes\n",
      "67\n",
      "finished import in 1.21 minutes\n",
      "finished subset in 0.00 minutes\n",
      "68\n",
      "finished import in 1.27 minutes\n",
      "finished subset in 0.00 minutes\n",
      "69\n",
      "finished import in 1.20 minutes\n",
      "finished subset in 0.00 minutes\n",
      "70\n",
      "finished import in 1.19 minutes\n",
      "finished subset in 0.00 minutes\n",
      "71\n",
      "finished import in 1.16 minutes\n",
      "finished subset in 0.00 minutes\n",
      "72\n",
      "finished import in 1.18 minutes\n",
      "finished subset in 0.00 minutes\n",
      "73\n",
      "finished import in 1.20 minutes\n",
      "finished subset in 0.00 minutes\n",
      "74\n",
      "finished import in 1.18 minutes\n",
      "finished subset in 0.00 minutes\n",
      "75\n",
      "finished import in 1.17 minutes\n",
      "finished subset in 0.00 minutes\n",
      "76\n",
      "finished import in 1.19 minutes\n",
      "finished subset in 0.00 minutes\n",
      "77\n",
      "finished import in 1.19 minutes\n",
      "finished subset in 0.00 minutes\n",
      "78\n",
      "finished import in 1.28 minutes\n",
      "finished subset in 0.00 minutes\n",
      "79\n",
      "finished import in 1.19 minutes\n",
      "finished subset in 0.00 minutes\n",
      "80\n",
      "finished import in 1.19 minutes\n",
      "finished subset in 0.00 minutes\n",
      "81\n",
      "finished import in 1.19 minutes\n",
      "finished subset in 0.00 minutes\n",
      "82\n",
      "finished import in 1.17 minutes\n",
      "finished subset in 0.00 minutes\n",
      "83\n",
      "finished import in 1.20 minutes\n",
      "finished subset in 0.00 minutes\n",
      "84\n",
      "finished import in 1.17 minutes\n",
      "finished subset in 0.00 minutes\n",
      "85\n",
      "finished import in 1.19 minutes\n",
      "finished subset in 0.00 minutes\n",
      "86\n",
      "finished import in 1.17 minutes\n",
      "finished subset in 0.00 minutes\n",
      "87\n",
      "finished import in 1.31 minutes\n",
      "finished subset in 0.00 minutes\n",
      "88\n",
      "finished import in 1.19 minutes\n",
      "finished subset in 0.00 minutes\n",
      "89\n",
      "finished import in 1.18 minutes\n",
      "finished subset in 0.00 minutes\n",
      "90\n",
      "finished import in 1.16 minutes\n",
      "finished subset in 0.00 minutes\n",
      "91\n",
      "finished import in 1.19 minutes\n",
      "finished subset in 0.00 minutes\n",
      "92\n",
      "finished import in 1.20 minutes\n",
      "finished subset in 0.00 minutes\n",
      "93\n",
      "finished import in 1.18 minutes\n",
      "finished subset in 0.00 minutes\n",
      "94\n",
      "finished import in 1.19 minutes\n",
      "finished subset in 0.00 minutes\n",
      "95\n",
      "finished import in 1.21 minutes\n",
      "finished subset in 0.00 minutes\n",
      "96\n",
      "finished import in 1.34 minutes\n",
      "finished subset in 0.00 minutes\n",
      "97\n",
      "finished import in 1.18 minutes\n",
      "finished subset in 0.00 minutes\n",
      "98\n",
      "finished import in 1.20 minutes\n",
      "finished subset in 0.00 minutes\n",
      "99\n",
      "finished import in 1.24 minutes\n",
      "finished subset in 0.00 minutes\n",
      "100\n",
      "finished import in 1.17 minutes\n",
      "finished subset in 0.00 minutes\n",
      "101\n",
      "finished import in 1.27 minutes\n",
      "finished subset in 0.00 minutes\n",
      "102\n",
      "finished import in 1.21 minutes\n",
      "finished subset in 0.00 minutes\n",
      "103\n",
      "finished import in 1.21 minutes\n",
      "finished subset in 0.00 minutes\n",
      "104\n",
      "finished import in 1.20 minutes\n",
      "finished subset in 0.00 minutes\n",
      "105\n",
      "finished import in 1.28 minutes\n",
      "finished subset in 0.00 minutes\n",
      "106\n",
      "finished import in 1.21 minutes\n",
      "finished subset in 0.00 minutes\n",
      "107\n",
      "finished import in 1.20 minutes\n",
      "finished subset in 0.00 minutes\n",
      "108\n",
      "finished import in 1.22 minutes\n",
      "finished subset in 0.00 minutes\n",
      "109\n",
      "finished import in 1.21 minutes\n",
      "finished subset in 0.00 minutes\n",
      "110\n",
      "finished import in 0.33 minutes\n",
      "finished subset in 0.00 minutes\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "df_full = pd.read_fwf('ghcnd_daily.dat', \n",
    "                      colspecs=column_specs,\n",
    "                      names=column_names, \n",
    "                      iterator=True,\n",
    "                      chunksize=1000000)\n",
    "                      \n",
    "                      # usecols=['id', 'year', 'month', 'element'] + [f'value{i}' for i in range(1, 32)])\n",
    "\n",
    "start = time.perf_counter()\n",
    "keeping = list()\n",
    "for idx, d in enumerate(df_full):\n",
    "    print(idx)\n",
    "    print(f\"finished import in {(time.perf_counter() - start) / 60:.2f} minutes\")\n",
    "    start = time.perf_counter()\n",
    "    d = d[d['element'] == \"TMAX\"]\n",
    "    keeping.append(d.copy())\n",
    "    #d = d.set_index('id')\n",
    "    #for s in my_stations: \n",
    "    #    try:\n",
    "    #        keeping.append(d.loc[s].copy())\n",
    "    #    except: \n",
    "    #        print(f\"{s} not in this block!\")\n",
    "    print(f\"finished subset in {(time.perf_counter() - start) / 60:.2f} minutes\")\n",
    "    start = time.perf_counter()\n",
    "    #del d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TextFileReader' object has no attribute 'dtypes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-89ee04c02a54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'TextFileReader' object has no attribute 'dtypes'"
     ]
    }
   ],
   "source": [
    "df_full.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5)** Check the values of your quality flags and decide whether you should keep all observations in your data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(6)** Now for each weather station, generate a separate plot of the daily temperatures over time. You should end up with a plot that looks something like this (though you'll be plotting daily max temps and daily min temps):\n",
    "\n",
    "![climate_plot_example](../images/climate_plot_example.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Want More Practice?\n",
    "\n",
    "Let's compare trends across countries!\n",
    "\n",
    "**(1)** Pick three countries and gather all the temperature reports for those countries. \n",
    "\n",
    "**(2)** For each day, average the max tempurature across all weather statements in each country to get a lower-variance estimate of overall temperature for each county. \n",
    "\n",
    "**(3)** Plot daily temperatures seperately for the three countries, and fit a linear model to see which has the greatest increases in temperature. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
